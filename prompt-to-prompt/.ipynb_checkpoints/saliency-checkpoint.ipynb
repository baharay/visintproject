{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(960, 1280)\n",
      "torch.Size([1, 3, 1280, 960])\n"
     ]
    }
   ],
   "source": [
    "#### Saliency notebook\n",
    "#### Takes an image and visualizes its saliency prediction\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as nnF\n",
    "import torchvision.transforms.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "def preprocess_img(img, i, j, h, w):\n",
    "    # i, j, h, w = 0, 0, 7.0/8 * 512, 14.0/8 * 512\n",
    "    img = F.crop(img, i, j, h, w)\n",
    "    #img = F.resize(img, 256, torchvision.transforms.InterpolationMode.BICUBIC)\n",
    "    img = F.to_tensor(img)\n",
    "    return img\n",
    "\n",
    "def normalize_imagenet(img):\n",
    "    img = F.normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    return img\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "img = Image.open('example_images/gnochi_mirror.jpeg')\n",
    "print(img.size)\n",
    "img_tensor = preprocess_img(img, 0, 0, img.size[1], img.size[0]).unsqueeze(0).to(DEVICE)\n",
    "print(img_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 40, 30])\n"
     ]
    }
   ],
   "source": [
    "# Load VGG and extract features for perceptual loss\n",
    "from utils import hook_model\n",
    "\n",
    "def load_vgg():\n",
    "    vgg = torchvision.models.vgg16(pretrained=True)\n",
    "    vgg.eval()\n",
    "    vgg.to(DEVICE)\n",
    "    return vgg\n",
    "\n",
    "# load the pretrained VGG\n",
    "vgg_model = load_vgg()\n",
    "\n",
    "# Hook the model to reach its middle layers\n",
    "vgg_hook, vgg_layers = hook_model(vgg_model, True)\n",
    "\n",
    "# Forward pass to activate the hook\n",
    "_ = vgg_model(normalize_imagenet(img_tensor))\n",
    "\n",
    "# Choose a layer from vgg_layers.keys()\n",
    "f_layer = 'features-30:MaxPool2d'\n",
    "\n",
    "# Extract the features\n",
    "feats = vgg_hook(f_layer).float()\n",
    "print(feats.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############PREDICT SALIENCY###################\n",
    "\n",
    "img = Image.open('example_images/gnochi_mirror.jpeg')\n",
    "print(img.size)\n",
    "\n",
    "img_tensor = preprocess_img(img, 0, 0, img.size[1], img.size[0]).unsqueeze(0).to(DEVICE)\n",
    "print(img_tensor.shape)\n",
    "print(\"PNAS Model\")\n",
    "from model import PNASModel\n",
    "model = PNASModel()\n",
    "model.load_state_dict(torch.load(\"/sinergia/bahar/visual-int/visintproject/prompt-to-prompt/saliency_backbone/saliency/salicon_pnas.pt\"))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "val_img_ids = os.listdir(args.val_img_dir)\n",
    "val_dataset = TestLoader(args.val_img_dir, val_img_ids)\n",
    "vis_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=args.no_workers)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
