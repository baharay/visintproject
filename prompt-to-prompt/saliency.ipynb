{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(960, 1280)\n",
      "torch.Size([1, 3, 1280, 960])\n"
     ]
    }
   ],
   "source": [
    "#### Saliency notebook\n",
    "#### Takes an image and visualizes its saliency prediction\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as nnF\n",
    "import torchvision.transforms.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "def preprocess_img(img, i, j, h, w):\n",
    "    # i, j, h, w = 0, 0, 7.0/8 * 512, 14.0/8 * 512\n",
    "    img = F.crop(img, i, j, h, w)\n",
    "    #img = F.resize(img, 256, torchvision.transforms.InterpolationMode.BICUBIC)\n",
    "    img = F.to_tensor(img)\n",
    "    return img\n",
    "\n",
    "def normalize_imagenet(img):\n",
    "    img = F.normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    return img\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "img = Image.open('example_images/gnochi_mirror.jpeg')\n",
    "print(img.size)\n",
    "img_tensor = preprocess_img(img, 0, 0, img.size[1], img.size[0]).unsqueeze(0).to(DEVICE)\n",
    "print(img_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aydemir/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aydemir/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 40, 30])\n"
     ]
    }
   ],
   "source": [
    "# Load VGG and extract features for perceptual loss\n",
    "from utils import hook_model\n",
    "\n",
    "def load_vgg():\n",
    "    vgg = torchvision.models.vgg16(pretrained=True)\n",
    "    vgg.eval()\n",
    "    vgg.to(DEVICE)\n",
    "    return vgg\n",
    "\n",
    "# load the pretrained VGG\n",
    "vgg_model = load_vgg()\n",
    "\n",
    "# Hook the model to reach its middle layers\n",
    "vgg_hook, vgg_layers = hook_model(vgg_model, True)\n",
    "\n",
    "# Forward pass to activate the hook\n",
    "_ = vgg_model(normalize_imagenet(img_tensor))\n",
    "\n",
    "# Choose a layer from vgg_layers.keys()\n",
    "f_layer = 'features-30:MaxPool2d'\n",
    "\n",
    "# Extract the features\n",
    "feats = vgg_hook(f_layer).float()\n",
    "print(feats.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(960, 1280)\n",
      "torch.Size([1, 3, 1280, 960])\n",
      "PNAS Model\n",
      "AAAAA\n",
      "96 96 54\n",
      "96 270 108\n",
      "270 540 216\n",
      "540 1080 216\n",
      "1080 1080 216\n",
      "1080 1080 216\n",
      "1080 1080 432\n",
      "1080 2160 432\n",
      "2160 2160 432\n",
      "2160 2160 432\n",
      "2160 2160 864\n",
      "2160 4320 864\n",
      "4320 4320 864\n",
      "4320 4320 864\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f6f82c85562b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTestLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_img_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_img_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#vis_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=args.no_workers)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sinergia/bahar/visual-int/visintproject/prompt-to-prompt/saliency_backbone/SimpleNet/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0ms0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpnas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "############PREDICT SALIENCY###################\n",
    "device = \"cuda\"\n",
    "img = Image.open('example_images/gnochi_mirror.jpeg')\n",
    "print(img.size)\n",
    "\n",
    "img_tensor = preprocess_img(img, 0, 0, img.size[1], img.size[0]).unsqueeze(0).to(DEVICE)\n",
    "print(img_tensor.shape)\n",
    "print(\"PNAS Model\")\n",
    "from saliency_backbone import PNAS\n",
    "from collections import OrderedDict\n",
    "\n",
    "from saliency_backbone.SimpleNet.model import PNASModel\n",
    "from saliency_backbone.SimpleNet.dataloader import TestLoader\n",
    "#from saliency_backbone.SimpleNet.model import PNASModel\n",
    "model = PNASModel()\n",
    "model_path = \"/sinergia/bahar/visual-int/visintproject/prompt-to-prompt/saliency_backbone/salicon_pnas.pt\"\n",
    "#model.load_state_dict(torch.load())\n",
    "state_dict = torch.load(model_path)\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "            if 'module'  in k:\n",
    "                k = k.replace('module.', '', )\n",
    "            new_state_dict[k] = v\n",
    "\n",
    "model.load_state_dict(new_state_dict)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "val_img_dir = \"/sinergia/bahar/visual-int/visintproject/prompt-to-prompt/example_images\"\n",
    "val_img_ids = os.listdir()\n",
    "val_dataset = TestLoader(val_img_dir, val_img_ids)\n",
    "#vis_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=args.no_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
